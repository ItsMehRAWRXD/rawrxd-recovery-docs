# Integration & Build Guide

## Overview

This guide explains how to build and integrate the refactored `InferenceEngine` with your project.

---

## Prerequisites

### Required Dependencies

```
Qt 5.15+ or Qt 6.x
  - Core (threading, signals/slots)
  - (Optional) GUI if using MainWindow integration

C++ Standard
  - C++17 or later (for std::optional, std::filesystem)
  - C++11 minimum works but recommended to use C++17

GGML/GGUF Support
  - ggml library (for tensor operations)
  - gguf-cpp headers (for format parsing)

Compiler Support
  - MSVC 2019+ (Windows)
  - GCC 9+ (Linux)
  - Clang 12+ (macOS)
```

### System Requirements

```
Minimum:
  - 2GB RAM (for small models)
  - 2 CPU cores
  - Storage: Model size + 2x during inference

Recommended:
  - 16GB+ RAM (for large models)
  - 8+ CPU cores
  - Storage: Model size + 4x for caching
  - GPU: NVIDIA/AMD (optional, requires additional setup)
```

---

## File Changes

### Modified Files

```
RawrXD-ModelLoader/src/qtapp/
â”œâ”€â”€ inference_engine.cpp          (575 lines, +~140 changed)
â”‚   â”œâ”€â”€ loadModel()              - Now loads real GGUF params
â”‚   â”œâ”€â”€ request()                - Uses generate() method
â”‚   â”œâ”€â”€ generate()               - Two-phase stateful inference
â”‚   â”œâ”€â”€ sampleNextToken()        - NEW: Top-P sampling
â”‚   â”œâ”€â”€ getRandomFloat()         - NEW: Thread-safe RNG
â”‚   â””â”€â”€ initializeTokenizer()    - Now loads real metadata
â”‚
â””â”€â”€ inference_engine.hpp         (202 lines, +~6 changed)
    â”œâ”€â”€ m_topP                   - NEW: sampling threshold
    â”œâ”€â”€ m_randomEngine           - NEW: MT19937 RNG
    â”œâ”€â”€ m_kvCacheReady           - NEW: cache state tracking
    â”œâ”€â”€ sampleNextToken()        - NEW: method signature
    â””â”€â”€ getRandomFloat()         - NEW: method signature
```

### No Changes Required To

```
Other source files (backward compatible):
  - gguf_loader.cpp/hpp
  - transformer_inference.cpp/hpp
  - bpe_tokenizer.cpp/hpp
  - sentencepiece_tokenizer.cpp/hpp
  - vocabulary_loader.cpp/hpp
  - quant_utils.cpp/hpp
  - (all other files)
```

---

## Build Instructions

### Using CMake (Recommended)

#### 1. Verify CMakeLists.txt

Ensure the project includes the modified files:

```cmake
# In your CMakeLists.txt
add_executable(inference_app
    src/qtapp/inference_engine.cpp
    src/qtapp/inference_engine.hpp
    src/qtapp/transformer_inference.cpp
    src/qtapp/gguf_loader.cpp
    # ... other files
)

target_link_libraries(inference_app
    Qt5::Core
    Qt5::Gui
    # ... other libraries
)

# Required C++ standard
set_property(TARGET inference_app PROPERTY CXX_STANDARD 17)
```

#### 2. Build Commands

```bash
# Create build directory
mkdir build
cd build

# Configure (release build recommended)
cmake .. -DCMAKE_BUILD_TYPE=Release

# Build (parallel make)
cmake --build . --parallel 8

# Alternative: traditional make
make -j8
```

#### 3. Verify Build

```bash
# Check for errors
cmake --build . 2>&1 | grep -i "error"

# If no errors found, build succeeded
echo "Build successful!"
```

### Using Qt Creator

1. **Open Project**
   - File â†’ Open File or Project
   - Select `CMakeLists.txt`

2. **Configure**
   - Select Release build type
   - Configure kit (MSVC/GCC/Clang)

3. **Build**
   - Build â†’ Build All
   - Or press Ctrl+Shift+B

4. **Check Compilation**
   - No errors in Issues tab

### Manual Build (g++)

```bash
# Compile with all necessary flags
g++ -std=c++17 -O3 \
    -I/path/to/Qt/include \
    -I/path/to/ggml/include \
    -I/path/to/project/src \
    -fPIC \
    src/qtapp/inference_engine.cpp \
    src/qtapp/transformer_inference.cpp \
    src/qtapp/gguf_loader.cpp \
    -o inference_app \
    -L/path/to/Qt/lib -lQt5Core -lQt5Gui \
    -L/path/to/ggml/lib -lggml

# Run
./inference_app
```

---

## Compilation Checklist

- âœ… No syntax errors
- âœ… All headers found
- âœ… All symbols resolved
- âœ… Correct C++ standard (C++17)
- âœ… Qt libraries linked
- âœ… GGML libraries linked
- âœ… No warnings (or acceptable warnings)

### Common Compilation Errors & Solutions

#### Error: "undefined reference to `m_loader->getParam'"

**Cause:** GGUFLoader doesn't have getParam method

**Solution:** Ensure GGUFLoader is updated to support:
```cpp
QVariant getParam(const QString& key, const QVariant& defaultValue);
QHash<QString, QByteArray> getTokenizerMetadata();
```

#### Error: "No such file or directory: mutex"

**Cause:** Missing `#include <mutex>`

**Solution:** Already included in updated inference_engine.cpp, verify includes:
```cpp
#include <mutex>  // For std::once_flag, std::call_once
```

#### Error: "std::mt19937 not found"

**Cause:** Missing `#include <random>`

**Solution:** Already included in updated inference_engine.cpp

#### Error: "QMutex is not defined"

**Cause:** Missing Qt includes or linking

**Solution:** Ensure:
```cpp
#include <QMutex>         // In header
target_link_libraries(... Qt5::Core)  // In CMakeLists
```

---

## Integration Points

### 1. GGUFLoader Interface

The refactored code expects these methods from `GGUFLoader`:

```cpp
class GGUFLoader {
public:
    // Existing methods
    bool isOpen() const;
    QStringList tensorNames() const;
    QByteArray inflateWeight(const QString& name) const;
    
    // New methods required
    QVariant getParam(const QString& key, const QVariant& defaultValue);
    QHash<QString, QByteArray> getTokenizerMetadata();
};
```

**If these don't exist:**

Add minimal implementations:
```cpp
QVariant GGUFLoader::getParam(const QString& key, const QVariant& defaultValue)
{
    // TODO: Parse GGUF metadata and return parameter
    // For now, return default
    return defaultValue;
}

QHash<QString, QByteArray> GGUFLoader::getTokenizerMetadata()
{
    // TODO: Extract tokenizer metadata from GGUF
    // Return BPE merges or SentencePiece model data
    return QHash<QString, QByteArray>();
}
```

### 2. Transformer Interface

The refactored code expects these methods from `TransformerInference`:

```cpp
class TransformerInference {
public:
    // Existing methods
    bool loadWeights(const QHash<QString, QByteArray>& tensorCache,
                    int nLayers, int nEmbd, int nHead, int nVocab);
    bool isReady() const;
    
    // Expected by new code
    std::vector<float> forward(const std::vector<int32_t>& tokens);
    
    // The forward() method should support:
    // - Single token: forward({token_id}) â†’ logits for next token
    // - Multiple tokens: forward({t1, t2, ...}) â†’ logits for next token (using cache)
};
```

**Current Implementation Check:**

If your `forward()` currently returns logits for ALL input tokens, update it to return logits for ONLY the next token (more efficient):

```cpp
// OLD (returns logits for all input tokens)
std::vector<std::vector<float>> forward(const std::vector<int32_t>& tokens)

// NEW (returns logits for only the last/next token)
std::vector<float> forward(const std::vector<int32_t>& tokens)
```

### 3. Tokenizer Classes

Ensure these classes support the required interfaces:

```cpp
class BPETokenizer {
public:
    bool loadFromGGUFMetadata(const QHash<QString, QByteArray>& metadata);
    std::vector<int32_t> encode(const QString& text);
    QString decode(const std::vector<int32_t>& tokens);
    bool isReady() const;
};

class SentencePieceTokenizer {
public:
    bool loadFromGGUFMetadata(const QHash<QString, QByteArray>& metadata);
    std::vector<int32_t> encode(const QString& text, bool add_bos, bool add_eos);
    QString decode(const std::vector<int32_t>& tokens, bool skip_special);
    bool isReady() const;
};
```

---

## Testing After Build

### 1. Basic Compilation Test

```cpp
#include "inference_engine.hpp"

int main() {
    InferenceEngine engine;
    qInfo() << "Compilation successful!";
    return 0;
}
```

### 2. Model Loading Test

```cpp
InferenceEngine engine;
bool loaded = engine.loadModel("models/llama2-7b.gguf");
qInfo() << "Model loaded:" << loaded;
qInfo() << "Memory usage:" << engine.memoryUsageMB() << "MB";
```

### 3. Generation Test

```cpp
std::vector<int32_t> tokens = engine.tokenize("Hello, world!");
std::vector<int32_t> result = engine.generate(tokens, 10);
QString text = engine.detokenize(result);
qInfo() << "Generated:" << text;
```

### 4. Sampling Test

```cpp
// Test different temperatures and Top-P values
engine.m_temperature = 0.7;
engine.m_topP = 0.9;
engine.request("Test prompt", 1);
```

---

## Performance Validation

### Benchmark Test

```cpp
#include <chrono>

void benchmarkGeneration(InferenceEngine& engine) {
    // Warmup
    auto warmupTokens = engine.tokenize("Warmup");
    engine.generate(warmupTokens, 5);
    
    // Benchmark
    auto prompt = engine.tokenize("The future of AI is");
    
    auto start = std::chrono::high_resolution_clock::now();
    auto result = engine.generate(prompt, 100);
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    qInfo() << "Generated" << result.size() << "tokens in" 
            << duration.count() << "ms";
    qInfo() << "Speed:" << engine.tokensPerSecond() << "tok/s";
}
```

### Expected Performance

```
Model          | Quantization | Speed (GPU) | Speed (CPU)
LLaMA 2 7B     | Q4_0         | ~150 tok/s  | ~30 tok/s
LLaMA 2 13B    | Q4_0         | ~80 tok/s   | ~15 tok/s
LLaMA 2 70B    | Q4_0         | ~15 tok/s   | ~3 tok/s
```

---

## Deployment

### Development Environment

```bash
# Development build (with debugging)
cmake .. -DCMAKE_BUILD_TYPE=Debug -DCMAKE_PREFIX_PATH=/path/to/Qt
cmake --build . --parallel 8
```

### Production Environment

```bash
# Release build (optimized, no debugging)
cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="-O3 -march=native"
cmake --build . --parallel 8
```

### Docker Deployment

```dockerfile
FROM ubuntu:22.04

# Install dependencies
RUN apt-get update && apt-get install -y \
    build-essential cmake \
    qt5-qmake qt5-default \
    libggml-dev

# Copy source
COPY . /src
WORKDIR /src/build

# Build
RUN cmake .. -DCMAKE_BUILD_TYPE=Release
RUN cmake --build . --parallel 8

# Run
ENTRYPOINT ["/src/build/inference_app"]
```

---

## Troubleshooting

### Runtime Issue: Model parameters not detected

**Problem:** All parameters show default values

**Solution:**
```
1. Verify GGUF file is valid
   $ file model.gguf
   
2. Check if GGUFLoader::getParam() is implemented
   
3. Add debug output
   qInfo() << "Loaded params:" << nLayers << nEmbd << nHead << nVocab;
```

### Runtime Issue: Generation produces repetitive text

**Problem:** Text keeps repeating phrases

**Solution:**
```cpp
// Ensure Top-P sampling is enabled (default)
engine.m_topP = 0.9;  // Not 1.0!

// Increase temperature
engine.m_temperature = 0.8;
```

### Runtime Issue: Out of memory

**Problem:** Crashes on large models

**Solution:**
```
1. Use higher quantization
   engine.setQuantMode("Q4_0");
   
2. Reduce context size in tokenize()
   
3. Use streaming generation (future enhancement)
```

### Runtime Issue: Thread safety warnings

**Problem:** Thread sanitizer warnings

**Solution:**
```
The implementation is thread-safe:
- Uses QMutexLocker for all state access
- Uses std::once_flag for RNG initialization
- All public methods are Qt slots (thread-safe)

If warnings persist, rebuild with correct sanitizer flags.
```

---

## Documentation Files

| Document | Purpose |
|----------|---------|
| `INFERENCE_ENGINE_REFACTORING.md` | Architecture & changes |
| `BEFORE_AFTER_COMPARISON.md` | Detailed comparisons |
| `INFERENCE_ENGINE_USAGE_GUIDE.md` | API reference |
| `TOP_P_SAMPLING_TECHNICAL_GUIDE.md` | Sampling algorithm |
| `REFACTORING_COMPLETE_SUMMARY.md` | Overall summary |
| `INTEGRATION_BUILD_GUIDE.md` | This file |

---

## Quick Reference

### Building

```bash
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --parallel 8
```

### Testing

```cpp
InferenceEngine engine("model.gguf");
engine.request("Test prompt", 1);
```

### Configuring

```cpp
engine.m_temperature = 0.7;  // Randomness
engine.m_topP = 0.9;          // Diversity
engine.setQuantMode("Q4_0");   // Speed vs. quality
```

---

## Support

For issues or questions:

1. Check documentation files
2. Review source code comments
3. Check compilation errors
4. Verify dependencies are installed
5. Test with simple examples first

Good luck! ðŸš€

